---
layout: post
title: 2020 NLP Wish List
excerpt: Things I would like to see happen in NLP in 2020.
---

All the cool kids on twitter are talking about their wishlists for NLP in 2020, and it got me thinking. Many of these overlap with stuff mentioned on twitter, but in my defense, Yoav had a very long list.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">these are some of my nlp wishlists for 2020. what are yours? <a href="https://t.co/Vnquwuad3D">https://t.co/Vnquwuad3D</a></p>&mdash; (((ل()(ل() &#39;yoav)))) (@yoavgo) <a href="https://twitter.com/yoavgo/status/1205991625228460033?ref_src=twsrc%5Etfw">December 14, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 

## Learning from small data
If I decided to learn a new language, and further decided that I would do it by examining only parallel texts, I wouldn't need 100M sentence pairs. I could make good headway by studying, say, parallel translations of the New Testament, which has less than 10,000 sentences. Current MT models can learn very little from 10,000 sentence pairs. What's the difference?

Similarly, I could take a person off the street and teach them to annotate NER in less than an hour. Why do our models need 200K+ tokens of annotated data?

## Understanding of multilingual contextual models
One of the most interesting things to come from the contextualization of NLP are the effortlessly multilingual models. Multilingual BERT just... works as a cross-lingual representation, even with no cross-lingual signal during pretraining. We saw this in the [BSNLP workshop](https://www.aclweb.org/anthology/W19-3710/), and in our [LORELEI work](https://cogcomp.seas.upenn.edu/papers/MTMWLYFSZYKHSSR19.pdf), and there have been [several](https://www.aclweb.org/anthology/D19-1077/) [other](https://www.aclweb.org/anthology/P19-1493/) [interesting](https://arxiv.org/abs/1901.07291) [works](https://arxiv.org/abs/1911.02116) [since](https://arxiv.org/abs/1912.07840). I'd love to see more understanding of *why* this works well, and how to exploit the mechanism for better multilingual representations.
 
## Reality checks against end users
Significance tests are important, sure, but I often wonder if end users would notice the difference between 93.0 and 93.5 on CoNLL2003. Similarly, at what point does a metric cross from being not useful to useful?

## Better definition of summarization
Summarization is an important and interesting problem, but it is also ill-defined. It seems to me that a summary depends on the *goal* or the *audience* of the summarization.

There's a neat example of this in a paper from [Salesforce](https://www.aclweb.org/anthology/D19-1051.pdf) at EMNLP 2019, where they ask humans to create summaries in two conditions, 1) unconstrained ("just write a summary") and 2) constrained, in which each summary is required to answer a small set of questions, created by annotators in a separate task. They write: *"Constrained summaries were more succinct and targeted, without sacrificing the natural flow of sentences. Unconstrained writers tended to write more verbose summaries that did not add information."* This suggests that some kind of target-focused or goal-oriented summarization would make for a better task.

## Green AI -- with a laptop
I've been intrigued for a while by a statement in the abstract of the [#GreenAI paper](https://arxiv.org/abs/1907.10597) from Roy Schwartz et al. They write: *"Our goal is to make AI both greener and more inclusive — enabling any inspired undergraduate with a laptop to write high-quality research papers."* This seems almost ridiculous in this age of quadrillion parameter models and hellabyte-scale corpora, but I think it could be a productive constraint. If you only had a laptop, you'd be forced to do something thoughtful and precise. Maybe we need a "laptops only" workshop.